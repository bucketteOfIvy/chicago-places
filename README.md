# The Big Picture

The United States is in an ongoing opioid epidemic. Since the 2000s, the opiodi death rate has steadily increased, reaching 32.6 deaths per 100,000 standard population in 2022 ([Spencer, 2024](https://www.cdc.gov/nchs/products/databriefs/db491.htm)). The severity of the epidemic has inspired a large research effort around understanding causes of opioid risk and provisioning harm reduction resources. From computational researchers, a common approach has been to attempt to nowcast opioid overdose events (OOE) and deaths, using a variety of traditional and big data sources as the predictive features. Results have varied, with some researchers finding low predictability ([Cuomo et al., 2023](https://doi.org/10.2196/42162); [Gavali et al., 2021](https://doi.org10.1109/BIBM52615.2021.9669486); [Schell et al., 2022](https://doi.org/10.1093/aje/kwab279)). while others using non-traditional datastes or more complex methods have achieved higher predictability ([Bozorgi et al., 2021](https://doi.org/10.1016/j.drugalcdep.2021.109143); [Li et al., 2022](https://doi.org/10.1016/j.healthplace.2022.102792)). One incredibly promising resource has been Google Streetview Imagery, which allows for computational evaluation of the context in which opioids are used ([Li et al., 2022](https://doi.org/10.1016/j.healthplace.2022.102792)).

The focus on nowcasting OOE and deaths is of vital importance and may better enable policy makers to accurately place resources in-the-moment, but an overemphasis on nowcasting in the computational literature has limited the ability of policy makers to place longer term harm reduction resources and investment. 

Towards alleviating this issue, we undertake a computational regionalization study in which we attempt to identify differing regions of opioid risk within the city of Chicago. For this study, we pull inspiration from two sources. First, we build our study around the opioid risk environment (ORE) framework developed by [Rhodes (2002)](https://doi.org/10.1016/S0955-3959(02)00007-5), which considers opioid risk as being constructed of four risk environments -- the social, economic, political, and physical environment -- and at two scales -- the micro and macro. Secondly, inspired by the success of [Li et al. (2022)]((https://doi.org/10.1016/j.healthplace.2022.102792)) in using Google StreetView imagery to predict opioid overdose events, we utilize Google StreetView Imagery to better characterize the physical risk environment. 

In sum, our study selects variables within the social, economic, and political environment, which we use to construct within environment regions. In the larger paper we validate these regions and compare their ORE framework predicted risks to narcotic arrests, a proxy for opioid overdose events and usage, and discover correlations between the spatial distribution of the modern opioid risk environment and historic redlining by the Homeowners Loan Corperation (HOLC). 


In fewer words and more colors, the following map of narcotic crimes (white points) overlaid on our three clusters sums up our results nicely:
![Side by side of of our three regionalized cluster maps with white narcotic dots depicted on top -- narcotic crime hotspots tend to occur within either highly a localized cluster, or within one large cluster.](https://github.com/bucketteOfIvy/chicago-places/blob/main/figures/narcotic_cluster_maps.svg)

# The Big Data Component 

Our study sought to characterize three aspects of the built environment -- perceived liveliness, perceived depressingness, and relative greenery -- using Google StreetView imagery. This entire project requires working with a large amount of image data, and the former two categories require working with the [place-pulse-2.0](https://paperswithcode.com/dataset/place-pulse-2-0) dataset of over 100,000 prelabeled images, meaning these variables cannot be created without high performance computing methods. Our code can be found in the [StreetviewDataMassaging](scripts/StreetviewDataMassaging) subdirectory of this repository, although we additionally walk through the 6 key steps (with links to the pertinent scripts) below. 

1. **Getting Points:** We randomly select 25,000 points from the Chicago street network and assign each point with a random heading ([pick_points.py](scripts/StreetviewDataMassaging/pick_points.py)). We then query the Google StreetView metadata API to determine which points actually have associated StreetView imagery, discovering that 24,240 images work ([validate_points.py](scripts/StreetviewDataMassaging/validate_points.py))

2. **Getting Images:** We create a lambda function which takes in a list of API calls, which it uses to retrieve Google StreetView imagery, with each image being stored in an S3 bucket ([lambda_function.py](scripts/StreetviewDataMassaging/aws_scrapers/deployment_packages/lambda_function.py)). After setting up our AWS environment ([scripts/StreetviewDataMassaging/aws_scrapers/initialize_aws.py]), we retrieve all 24,240 images through Lambda step functions ([pull_images.py](scripts/StreetviewDataMassaging/aws_scrapers/pull_images.py)). Notably, potentially due to constraints on Step Functions called from AWS Student account, we are forced to call our Step Functions multiple times, so that our data is split into "superbatches" (e.g. doing 100 superbatches in which we provide a step function with 2,500 requests to split between 10 lambda functions). Once our scrapers have ran, we kill our lambda function and step function ([teardown_aws.py](scripts/StreetviewDataMassaging/aws_scrapers/teardown_aws.py)).

3. **Moving to Midway:** Due to reasons of researcher preference, we opt to do as much of our analysis on the Midway clusters as possible. As such, we retrieve our StreetView imagery from S3 using boto3 ([save_images_to_local.py](scripts/StreetviewDataMassaging/aws_scrapers/save_images_to_local.py)), and additionally retrieve Place Pulse 2.0 imagery -- used in the next steps -- from UChicago Box, where they were placed for long term storage and ease of access ([download_place_pulse.py](scripts/StreetviewDataMassaging/download_place_pulse.py)).

4. **Image Segmentation:** We semantically segment our 24,240 images using a [pretrained semantic segmentation model from MIT](https://github.com/CSAILVision/semantic-segmentation-pytorch) ([download_model.sh](scripts/StreetviewDataMassaging/segmentation/download_model.sh) [segment_script.py](scripts/StreetviewDataMassaging/segmentation/segment_script.py) [segmentation.sbatch](scripts/StreetviewDataMassaging/segmentation/segmentation.sbatch)). We additionally semantically segment our Place Pulse 2.0 imagery using the same model ([place_pulse_segment](scripts/StreetviewDataMassaging/segmentation/place_pulse_segment.py) [place_pulse_segment.sbatch](scripts/StreetviewDataMassaging/segmentation/place_pulse_segment.sbatch)).

5. **Final Feature Creation:** The feature creation portion of our pipeline is forked.
    1. **Relative Greenery Index:** We create our relative greenery index in two steps. First, we calculate and absolute greenery index $G_{abs}$ for each image by taking the total number of pixels in the image classified as one of "grass", "field", "flower", "hill", or "tree" ([make_greenery_index.py](scripts/StreetviewDataMassaging/make_greenery_index.py)). Then, we calculate the relative greenery index of an image through the following equation:
    $$g_{img}=\frac{G_{img}-\min_{j\in\text{Images}}G_j}{\max_{j\in\text{Images}}G_j-\min_{j\in\text{Images}}G_j}$$
    2. **Percieved Aspects of the Built Environment:** The Place-Pulse 2.0 Survey released a series of over 100,000 StreetView images -- which we segmented in step 4 -- along with associated perceived liveliness and boringness values. We use pyspark to construct and validate a few basic Machine Learning models in an attempt to generalize these predictions to our corpus ([extract_metadata.py](scripts/StreetviewDataMassaging/ml_pipeline/extract_metadata.py) [models.sbatch](scripts/StreetviewDataMassaging/ml_pipleine/models.py) [models.sbatch](scripts/StreetviewDataMassaging/models.sbatch)). However, due to poor model performance -- the RMSE error of our resultant models is nearly one standard deviation in size -- we decline to label our 24,240 Chicago images.

6. **Long Term Data Storage:** In order to increase the long term replicability of this project, we migrate our data from S3 to UChicago Box (utilizing the download script and a manual upload to box). Additionally, we include our Place-Pulse 2.0 imagery, allowing users to download the data to recreate our ML pipeline with less fear of link rot. After doing so, we called [teardown_bucket.py](scripts/StreetviewDataMassaging/aws_scrapers/teardown_bucket.py) to kill our s3 bucket.

# Next Steps and Future Improvements

While we only decided to scale up to one of our resultant datasets, this project has still been a large success: we have a highly scaleable and segmentable pipeline which--while it doesn't produce the _nicest_ predictions yet--should be editable so that it does. In fact, our low predictabiltiy is likely due to our choice of semantic segmentation model, with the chosen model containing mostly terms which are better suited towards classifying interior scenes than outdoor scenes. However, our code shouldn't take more than minor edits to load and utilize different pretrained pytorch models, so this should be an issue that can easily revised during the summer.

Additionally, the scalability of our pipeline means we can easily deploy this workflow in other cities. Deploying this pipeline in New York City, for example, would only require the acquisition of another city street shapefile and additional Google StreetView API credits. Scaleability thus opens the door for future work comparing the risk environment between multiple settings and may enable a nice MA Thesis to emerge from this project.

<img src="figures/relative_greenery_map.png" alt="A map of relative greenery in Chicago; the areas of lower greenery -- roadways -- are highly visible on the map" width="600px" height="800px">

# Copying

This project is licensed under the GNU General Public License v3.0. The full license can be read [here](COPYING).
